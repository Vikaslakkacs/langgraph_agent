{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub ## For defining prompts\n",
    "from langchain.agents import create_openai_functions_agent##Creates agents from openai\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults ## Tavily search results\n",
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Lang chain agent\n",
    "* We will define langchain agent with tools available.\n",
    "    * Here the tools are Tavilysearch and vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create Vector DB\n",
    "from langchain_community.document_loaders import WebBaseLoader # Loads data from a web page\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # Split the characters into chunks\n",
    "from langchain_community.vectorstores import FAISS # ccreates a vector store\n",
    "from langchain_openai import OpenAIEmbeddings # Embedding of text\n",
    "\n",
    "\n",
    "loader= WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs= loader.load()\n",
    "documents= RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "\n",
    "vector= FAISS.from_documents(documents=documents, embedding=OpenAIEmbeddings())\n",
    "retriever= vector.as_retriever()\n",
    "\n",
    "##3 Tavily Search\n",
    "search= TavilySearchResults(max_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create retriever tool for retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool= create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name= \"langsmith_retriever\",\n",
    "    description=\"Provides information about langsmith and its components\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools= [search, retriever_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an agent now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "\n",
    "## Pull prompt\n",
    "prompt= hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "### Choose LLm to drive agent\n",
    "llm= ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "\n",
    "# As we are using openai we will construct agent with openai\n",
    "agent_runnable= create_openai_functions_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining graph state\n",
    "Generally a default/basic/traditional langchain agent has below attributes\n",
    "1. input: This is the query/information that we will get from user\n",
    "2. chat_history: This stores history of the chat\n",
    "3. Intermediate_steps: The list of actions and corresponding observations that agen consider over time. This is updated at each iteration of the agent.\n",
    "4. agent_outcome: This is response from the agent after going through nodes via edges. This is a response either from AgentAction and AgentFinish. AgentExecutor should finish when it is AgentFinish otherwise it should call requested tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define Schema for Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, TypedDict, Union\n",
    "## Define AgentACtion and AgentFinish\n",
    "from langchain_core.agents import AgentAction,AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "##Define schema class \n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: list[BaseMessage]\n",
    "    ## Beside AgentAction an AgentFinish it can return None\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None]\n",
    "    ###We will mention that Corresponding action should be added to \n",
    "    ### intermediate steps and this is via oeprator.add\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define nodes\n",
    "* We define nodes in the graph. Nodes can be Runnables or a function that can return customized result.\n",
    "* There are two main nodes that we definetly need.\n",
    "    * The agent: This is responsible to decide which action to take\n",
    "    * function to invoke tools: IF agent decides to take action, for that we define function what to execute\n",
    "\n",
    "* We define edgees which give information of the proces flow. It defines where to flow and which function to execute.\n",
    "    * Conditional edge: With If else\n",
    "    * Normal edge: Normal straight edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.agents import AgentFinish\n",
    "## Define tool executor\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "## This is a helper wheere it takes all the details of tools.\n",
    "## and invokes which ever tool we want when we pass respective tool.\n",
    "tool_executor= ToolExecutor(tools)\n",
    "\n",
    "## Define Agent\n",
    "def run_agent(data):\n",
    "    ## This is the initial step\n",
    "    ## Hence we invoke the data\n",
    "    agent_outcome= agent_runnable.invoke(data)\n",
    "    return {\"agent_outcome\": agent_outcome}\n",
    "\n",
    "## Once we run agent then it will return tool name to run\n",
    "## Create fuction to execute that tool name\n",
    "def execute_tool(data):\n",
    "    ## agent_outcome will return agent action, agentEnd as defined in schema\n",
    "    ## We will choose the latest one\n",
    "    agent_action= data['agent_outcome']\n",
    "    output= tool_executor.invoke(agent_action)\n",
    "    ## return it to intermediate steps which stores all the details\n",
    "    return {\"intermediate_steps\": [(agent_action, str(output))]}\n",
    "\n",
    "## Defining logic weather the process has to continue or end based on process flow\n",
    "def should_continue(data):\n",
    "    ##Check the outcome variable an decide whether to continue or to end the flow\n",
    "    if isinstance(data['agent_outcome'], AgentFinish):\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now its time to define graph\n",
    "* We will pull all the information that we have designed to crate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "## Define State graph\n",
    "workflow= StateGraph(AgentState)\n",
    "\n",
    "## Define initial two nodes that play with in the middle of the process\n",
    "workflow.add_node(\"agent\", run_agent)\n",
    "workflow.add_node(\"action\", execute_tool)\n",
    "\n",
    "## Define edges where the node has to start\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "## WE will now add conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \n",
    "    \"agent\",\n",
    "    \n",
    "    should_continue,\n",
    "    \n",
    "    ## IF above should-continue returns below values\n",
    "    ## The keys are strings and values are nodes in the below result\n",
    "    {\n",
    "        # IF the response is tools then we continue \n",
    "        \"continue\": \"action\",\n",
    "        # else\n",
    "        \"end\": END,\n",
    "    }\n",
    "    \n",
    ")\n",
    "\n",
    "## Once tools are called from the conditional edge\n",
    "## Then we will call agent node again to invoke.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "## Once we get the flow we will compile it.\n",
    "## This will be compiled to a langchainrunnable\n",
    "## This can be used as any other general runnables\n",
    "app= workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_outcome': AgentActionMessageLog(tool='tavily_search_results_json', tool_input={'query': 'weather in San Francisco'}, log=\"\\nInvoking: `tavily_search_results_json` with `{'query': 'weather in San Francisco'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}}, response_metadata={'finish_reason': 'function_call', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-26e4e90a-7239-45ff-b4b9-4eb98c8fad0f-0')])}\n",
      "'-----'\n",
      "{'intermediate_steps': [(AgentActionMessageLog(tool='tavily_search_results_json', tool_input={'query': 'weather in San Francisco'}, log=\"\\nInvoking: `tavily_search_results_json` with `{'query': 'weather in San Francisco'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}}, response_metadata={'finish_reason': 'function_call', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-26e4e90a-7239-45ff-b4b9-4eb98c8fad0f-0')]),\n",
      "                         \"[{'url': 'https://www.weatherapi.com/', 'content': \"\n",
      "                         '\"{\\'location\\': {\\'name\\': \\'San Francisco\\', '\n",
      "                         \"'region': 'California', 'country': 'United States of \"\n",
      "                         \"America', 'lat': 37.78, 'lon': -122.42, 'tz_id': \"\n",
      "                         \"'America/Los_Angeles', 'localtime_epoch': \"\n",
      "                         \"1722578809, 'localtime': '2024-08-01 23:06'}, \"\n",
      "                         \"'current': {'last_updated_epoch': 1722578400, \"\n",
      "                         \"'last_updated': '2024-08-01 23:00', 'temp_c': 14.7, \"\n",
      "                         \"'temp_f': 58.4, 'is_day': 0, 'condition': {'text': \"\n",
      "                         \"'Partly Cloudy', 'icon': \"\n",
      "                         \"'//cdn.weatherapi.com/weather/64x64/night/116.png', \"\n",
      "                         \"'code': 1003}, 'wind_mph': 5.8, 'wind_kph': 9.4, \"\n",
      "                         \"'wind_degree': 245, 'wind_dir': 'WSW', \"\n",
      "                         \"'pressure_mb': 1018.0, 'pressure_in': 30.06, \"\n",
      "                         \"'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 92, \"\n",
      "                         \"'cloud': 45, 'feelslike_c': 14.3, 'feelslike_f': \"\n",
      "                         \"57.8, 'windchill_c': 14.3, 'windchill_f': 57.8, \"\n",
      "                         \"'heatindex_c': 14.7, 'heatindex_f': 58.4, \"\n",
      "                         \"'dewpoint_c': 13.3, 'dewpoint_f': 55.9, 'vis_km': \"\n",
      "                         \"10.0, 'vis_miles': 6.0, 'uv': 1.0, 'gust_mph': 9.1, \"\n",
      "                         '\\'gust_kph\\': 14.7}}\"}]')]}\n",
      "'-----'\n",
      "{'agent_outcome': AgentFinish(return_values={'output': 'The current weather in San Francisco is partly cloudy with a temperature of 58.4°F (14.7°C). The wind speed is 5.8 mph (9.4 kph) coming from the WSW direction. The humidity is at 92% and the visibility is 6.0 miles.'}, log='The current weather in San Francisco is partly cloudy with a temperature of 58.4°F (14.7°C). The wind speed is 5.8 mph (9.4 kph) coming from the WSW direction. The humidity is at 92% and the visibility is 6.0 miles.')}\n",
      "'-----'\n"
     ]
    }
   ],
   "source": [
    "inputs= {\"input\": \"what is weather in SF?\", \"chat_history\": []}\n",
    "\n",
    "from pprint import pprint\n",
    "for result in app.stream(inputs):\n",
    "    pprint(list(result.values())[0])\n",
    "    pprint(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.weatherapi.com/',\n",
       "  'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1722579213, 'localtime': '2024-08-01 23:13'}, 'current': {'last_updated_epoch': 1722578400, 'last_updated': '2024-08-01 23:00', 'temp_c': 14.7, 'temp_f': 58.4, 'is_day': 0, 'condition': {'text': 'Partly Cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/night/116.png', 'code': 1003}, 'wind_mph': 5.8, 'wind_kph': 9.4, 'wind_degree': 245, 'wind_dir': 'WSW', 'pressure_mb': 1018.0, 'pressure_in': 30.06, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 92, 'cloud': 45, 'feelslike_c': 14.3, 'feelslike_f': 57.8, 'windchill_c': 14.3, 'windchill_f': 57.8, 'heatindex_c': 14.7, 'heatindex_f': 58.4, 'dewpoint_c': 13.3, 'dewpoint_f': 55.9, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 1.0, 'gust_mph': 9.1, 'gust_kph': 14.7}}\"}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.invoke(\"What is weather in SF?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'agent_outcome': AgentActionMessageLog(tool='langsmith_retriever', tool_input={'query': 'How to create dataset in langsmith?'}, log=\"\\nInvoking: `langsmith_retriever` with `{'query': 'How to create dataset in langsmith?'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"How to create dataset in langsmith?\"}', 'name': 'langsmith_retriever'}}, response_metadata={'finish_reason': 'function_call', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-b8f346e3-9ea5-4502-822f-b6b031aa2a30-0')])}]\n",
      "'-----'\n",
      "[{'intermediate_steps': [(AgentActionMessageLog(tool='langsmith_retriever', tool_input={'query': 'How to create dataset in langsmith?'}, log=\"\\nInvoking: `langsmith_retriever` with `{'query': 'How to create dataset in langsmith?'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"How to create dataset in langsmith?\"}', 'name': 'langsmith_retriever'}}, response_metadata={'finish_reason': 'function_call', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-b8f346e3-9ea5-4502-822f-b6b031aa2a30-0')]),\n",
      "                          'Skip to main contentGo to API '\n",
      "                          'DocsSearchRegionUSEUGo to AppQuick '\n",
      "                          'startTutorialsHow-to '\n",
      "                          'guidesConceptsReferencePricingSelf-hostingLangGraph '\n",
      "                          'CloudQuick startOn this pageGet started with '\n",
      "                          'LangSmithLangSmith is a platform for building '\n",
      "                          'production-grade LLM applications. It allows you to '\n",
      "                          'closely monitor and evaluate your application, so '\n",
      "                          'you can ship quickly and with confidence. Use of '\n",
      "                          'LangChain is not necessary - LangSmith works on its '\n",
      "                          'own!1. Install LangSmith\\u200bPythonTypeScriptpip '\n",
      "                          'install -U langsmithyarn add langsmith2. Create an '\n",
      "                          'API key\\u200bTo create an API key head to the '\n",
      "                          'Settings page. Then click Create API Key.3. Set up '\n",
      "                          'your environment\\u200bShellexport '\n",
      "                          'LANGCHAIN_TRACING_V2=trueexport '\n",
      "                          'LANGCHAIN_API_KEY=<your-api-key># The below '\n",
      "                          \"examples use the OpenAI API, though it's not \"\n",
      "                          'necessary in generalexport '\n",
      "                          'OPENAI_API_KEY=<your-openai-api-key>4. Log your '\n",
      "                          'first trace\\u200bWe provide multiple ways to log '\n",
      "                          \"traces to LangSmith. Below, we'll highlight\\n\"\n",
      "                          '\\n'\n",
      "                          'Get started with LangSmith | 🦜️🛠️ LangSmith\\n'\n",
      "                          '\\n'\n",
      "                          '\"revision_id\": \"beta\"    },)import { Client, Run, '\n",
      "                          'Example } from \"langsmith\";import { evaluate } from '\n",
      "                          '\"langsmith/evaluation\";import { EvaluationResult } '\n",
      "                          'from \"langsmith/evaluation\";const client = new '\n",
      "                          'Client();// Define dataset: these are your test '\n",
      "                          'casesconst datasetName = \"Sample Dataset\";const '\n",
      "                          'dataset = await client.createDataset(datasetName, '\n",
      "                          '{  description: \"A sample dataset in '\n",
      "                          'LangSmith.\",});await client.createExamples({  '\n",
      "                          'inputs: [    { postfix: \"to LangSmith\" },    { '\n",
      "                          'postfix: \"to Evaluations in LangSmith\" },  ],  '\n",
      "                          'outputs: [    { output: \"Welcome to LangSmith\" '\n",
      "                          '},    { output: \"Welcome to Evaluations in '\n",
      "                          'LangSmith\" },  ],  datasetId: dataset.id,});// '\n",
      "                          'Define your evaluatorconst exactMatch = async (  '\n",
      "                          'run: Run,  example: Example): '\n",
      "                          'Promise<EvaluationResult> => {  return {    key: '\n",
      "                          '\"exact_match\",    score: run.outputs?.output === '\n",
      "                          'example?.outputs?.output,  };};await evaluate(  '\n",
      "                          '(input: { postfix: string }) => ({ output: `Welcome '\n",
      "                          '${input.postfix}` }),  {    data: datasetName,    '\n",
      "                          'evaluators:\\n'\n",
      "                          '\\n'\n",
      "                          'description=\"A sample dataset in '\n",
      "                          'LangSmith.\")client.create_examples(    '\n",
      "                          'inputs=[        {\"postfix\": \"to LangSmith\"},        '\n",
      "                          '{\"postfix\": \"to Evaluations in LangSmith\"},    '\n",
      "                          '],    outputs=[        {\"output\": \"Welcome to '\n",
      "                          'LangSmith\"},        {\"output\": \"Welcome to '\n",
      "                          'Evaluations in LangSmith\"},    ],    '\n",
      "                          'dataset_id=dataset.id,)# Define your evaluatordef '\n",
      "                          'exact_match(run, example):    return {\"score\": '\n",
      "                          'run.outputs[\"output\"] == '\n",
      "                          'example.outputs[\"output\"]}experiment_results = '\n",
      "                          'evaluate(    lambda input: \"Welcome \" + '\n",
      "                          \"input['postfix'], # Your AI system goes here    \"\n",
      "                          'data=dataset_name, # The data to predict and grade '\n",
      "                          'over    evaluators=[exact_match], # The evaluators '\n",
      "                          'to score the results    '\n",
      "                          'experiment_prefix=\"sample-experiment\", # The name '\n",
      "                          'of the experiment    metadata={      \"version\": '\n",
      "                          '\"1.0.0\",      \"revision_id\": \"beta\"    },)import { '\n",
      "                          'Client, Run, Example } from \"langsmith\";import { '\n",
      "                          'evaluate } from \"langsmith/evaluation\";import { '\n",
      "                          'EvaluationResult } from '\n",
      "                          '\"langsmith/evaluation\";const client = new')]}]\n",
      "'-----'\n",
      "[{'agent_outcome': AgentFinish(return_values={'output': 'To create a dataset in LangSmith, you can follow these steps:\\n\\n1. Install LangSmith:\\n   - Python: `pip install -U langsmith`\\n   - TypeScript: `yarn add langsmith`\\n\\n2. Create an API key:\\n   - Head to the Settings page and click on Create API Key.\\n\\n3. Set up your environment:\\n   - Set environment variables:\\n     ```\\n     export LANGCHAIN_TRACING_V2=true\\n     export LANGCHAIN_API_KEY=<your-api-key>\\n     export OPENAI_API_KEY=<your-openai-api-key>\\n     ```\\n\\n4. Log your first trace:\\n   - Use the provided code snippets to define your dataset, create examples, and define your evaluator.\\n\\nHere is an example code snippet for creating a dataset in LangSmith:\\n```python\\nimport { Client, Run, Example } from \"langsmith\";\\nimport { evaluate } from \"langsmith/evaluation\";\\nimport { EvaluationResult } from \"langsmith/evaluation\";\\n\\nconst client = new Client();\\n\\n// Define dataset: these are your test cases\\nconst datasetName = \"Sample Dataset\";\\nconst dataset = await client.createDataset(datasetName, { description: \"A sample dataset in LangSmith.\" });\\n\\nawait client.createExamples({\\n  inputs: [\\n    { postfix: \"to LangSmith\" },\\n    { postfix: \"to Evaluations in LangSmith\" },\\n  ],\\n  outputs: [\\n    { output: \"Welcome to LangSmith\" },\\n    { output: \"Welcome to Evaluations in LangSmith\" },\\n  ],\\n  datasetId: dataset.id,\\n});\\n\\n// Define your evaluator\\nconst exactMatch = async (run: Run, example: Example): Promise<EvaluationResult> => {\\n  return {\\n    key: \"exact_match\",\\n    score: run.outputs?.output === example?.outputs?.output,\\n  };\\n};\\n\\nawait evaluate(\\n  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),\\n  { data: datasetName, evaluators: [exactMatch] }\\n);\\n```\\n\\nYou can customize the dataset name, examples, and evaluators based on your specific requirements.'}, log='To create a dataset in LangSmith, you can follow these steps:\\n\\n1. Install LangSmith:\\n   - Python: `pip install -U langsmith`\\n   - TypeScript: `yarn add langsmith`\\n\\n2. Create an API key:\\n   - Head to the Settings page and click on Create API Key.\\n\\n3. Set up your environment:\\n   - Set environment variables:\\n     ```\\n     export LANGCHAIN_TRACING_V2=true\\n     export LANGCHAIN_API_KEY=<your-api-key>\\n     export OPENAI_API_KEY=<your-openai-api-key>\\n     ```\\n\\n4. Log your first trace:\\n   - Use the provided code snippets to define your dataset, create examples, and define your evaluator.\\n\\nHere is an example code snippet for creating a dataset in LangSmith:\\n```python\\nimport { Client, Run, Example } from \"langsmith\";\\nimport { evaluate } from \"langsmith/evaluation\";\\nimport { EvaluationResult } from \"langsmith/evaluation\";\\n\\nconst client = new Client();\\n\\n// Define dataset: these are your test cases\\nconst datasetName = \"Sample Dataset\";\\nconst dataset = await client.createDataset(datasetName, { description: \"A sample dataset in LangSmith.\" });\\n\\nawait client.createExamples({\\n  inputs: [\\n    { postfix: \"to LangSmith\" },\\n    { postfix: \"to Evaluations in LangSmith\" },\\n  ],\\n  outputs: [\\n    { output: \"Welcome to LangSmith\" },\\n    { output: \"Welcome to Evaluations in LangSmith\" },\\n  ],\\n  datasetId: dataset.id,\\n});\\n\\n// Define your evaluator\\nconst exactMatch = async (run: Run, example: Example): Promise<EvaluationResult> => {\\n  return {\\n    key: \"exact_match\",\\n    score: run.outputs?.output === example?.outputs?.output,\\n  };\\n};\\n\\nawait evaluate(\\n  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),\\n  { data: datasetName, evaluators: [exactMatch] }\\n);\\n```\\n\\nYou can customize the dataset name, examples, and evaluators based on your specific requirements.')}]\n",
      "'-----'\n"
     ]
    }
   ],
   "source": [
    "inputs= {\"input\": \"How to create dataset in langsmith?\", \"chat_history\": []}\n",
    "\n",
    "from pprint import pprint\n",
    "for result in app.stream(inputs):\n",
    "    pprint(list(result.values())[0])\n",
    "    pprint(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
